#!/bin/bash
#SBATCH --job-name=aorl
#SBATCH --open-mode=append
#SBATCH -o /global/scratch/users/jenniferzhao/logs/%A_%a.out
#SBATCH -e /global/scratch/users/jenniferzhao/logs/%A_%a.err
#SBATCH --time=1:00:00
#SBATCH --mem=64G
#SBATCH --cpus-per-task=4
#SBATCH --gres=gpu:A5000:1
#SBATCH --account=co_rail
#SBATCH --partition=savio4_gpu
#SBATCH --qos=rail_gpu4_low
#SBATCH --requeue
#SBATCH --array=1-12%16

TASK_ID=$((SLURM_ARRAY_TASK_ID-1))
PARALLEL_N=2
JOB_N=24

COM_ID_S=$((TASK_ID * PARALLEL_N + 1))
source ~/.bashrc
micromamba activate aorl
export PYTHONPATH="../:${PYTHONPATH}"

declare -a commands=(
 [1]='python3 e2e.py --run_group e2e_maze_4_3 --seed 0 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 100000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 50 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid f302l3jr'
 [2]='python3 e2e.py --run_group e2e_maze_4_3 --seed 0 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 100000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 100 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid 7zlu7rvk'
 [3]='python3 e2e.py --run_group e2e_maze_4_3 --seed 0 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 100000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 200 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid it3ru10q'
 [4]='python3 e2e.py --run_group e2e_maze_4_3 --seed 0 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 1000000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 50 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid 7e0mnhvj'
 [5]='python3 e2e.py --run_group e2e_maze_4_3 --seed 0 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 1000000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 100 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid lluxbxd0'
 [6]='python3 e2e.py --run_group e2e_maze_4_3 --seed 0 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 1000000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 200 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid lnxlxdts'
 [7]='python3 e2e.py --run_group e2e_maze_4_3 --seed 1 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 100000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 50 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid 1cbzfq6k'
 [8]='python3 e2e.py --run_group e2e_maze_4_3 --seed 1 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 100000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 100 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid ujlufa3r'
 [9]='python3 e2e.py --run_group e2e_maze_4_3 --seed 1 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 100000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 200 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid al0d5vf9'
 [10]='python3 e2e.py --run_group e2e_maze_4_3 --seed 1 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 1000000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 50 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid 58td1a7m'
 [11]='python3 e2e.py --run_group e2e_maze_4_3 --seed 1 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 1000000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 100 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid 3pqgexet'
 [12]='python3 e2e.py --run_group e2e_maze_4_3 --seed 1 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 1000000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 200 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid hqmt1mte'
 [13]='python3 e2e.py --run_group e2e_maze_4_3 --seed 2 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 100000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 50 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid 9zjeqkwj'
 [14]='python3 e2e.py --run_group e2e_maze_4_3 --seed 2 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 100000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 100 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid lhqm35bh'
 [15]='python3 e2e.py --run_group e2e_maze_4_3 --seed 2 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 100000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 200 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid hztp8zxa'
 [16]='python3 e2e.py --run_group e2e_maze_4_3 --seed 2 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 1000000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 50 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid wge4dq49'
 [17]='python3 e2e.py --run_group e2e_maze_4_3 --seed 2 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 1000000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 100 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid xpsxh78z'
 [18]='python3 e2e.py --run_group e2e_maze_4_3 --seed 2 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 1000000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 200 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid 7woj8lf7'
 [19]='python3 e2e.py --run_group e2e_maze_4_3 --seed 3 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 100000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 50 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid eoy8v5vl'
 [20]='python3 e2e.py --run_group e2e_maze_4_3 --seed 3 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 100000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 100 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid sxkl9trl'
 [21]='python3 e2e.py --run_group e2e_maze_4_3 --seed 3 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 100000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 200 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid i4tvbzae'
 [22]='python3 e2e.py --run_group e2e_maze_4_3 --seed 3 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 1000000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 50 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid k85wf13n'
 [23]='python3 e2e.py --run_group e2e_maze_4_3 --seed 3 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 1000000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 100 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid s69jif3u'
 [24]='python3 e2e.py --run_group e2e_maze_4_3 --seed 3 --env_name antmaze-medium-navigate-oraclerep-v0 --agent ../agents/gcfql.py --dataset_dir ../../scratch/data/antmaze-medium-navigate-v0 --train_data_size 1000000 --save_dir ../../scratch --offline_steps 100 --save_interval 50 --log_interval 100 --eval_interval 50 --eval_episodes 0 --video_episodes 0 --agent.alpha 300 --agent.actor_type best-of-n --agent.train_goal_proposer=True --agent.goal_proposer_type default --agent.subgoal_steps 200 --agent.actor_hidden_dims 512,512,512,512 --agent.value_hidden_dims 512,512,512,512 --agent.batch_size 256 --agent.num_actions 8 --agent.num_qs 10 --agent.q_agg mean --agent.discount 0.995 --data_option datafuncs/withsubgoal.py --data_option.collection_steps 100 --data_option.save_data_interval 50 --data_option.plot_interval 50 --data_option.max_episode_steps 20  --wbid zae451xb'
)

cd /home/jennifer/aorl/horizon_reduction

parallel --delay 20 --linebuffer -j 2 {1} ::: "${commands[@]:$COM_ID_S:$PARALLEL_N}"
